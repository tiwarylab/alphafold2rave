[Model Parameters]
# Dimension of RC or bottleneck
d = [2]
# Encoder type (Linear or Nonlinear)
encoder_type = Linear
# Number of nodes in each hidden layer of the encoder
neuron_num1 = [32]
# Number of nodes in each hidden layer of the decoder
neuron_num2 = [32]
[Training Parameters]
batch_size = 300
# Threshold in terms of the change of the predicted state population for measuring the convergence of training
threshold = 0.01
# Number of epochs with the change of the state population smaller than the threshold after which this iteration of training finishes
patience = 2
# Number of refinements
refinements = 8
# Period of learning rate decay
lr_scheduler_step_size = 5
# Multiplicative factor of learning rate decay. Default: 1 (No learning rate decay)
lr_scheduler_gamma = 0.95
# By default, we save the model every 10000 steps
log_interval = 10000
# Initial learning rate of Adam optimizer
learning_rate = [1e-2]
# Hyper-parameter beta
beta = [1e-2]

[Other Controls]
# Random seed
seed = [0]
# Whether to refine the labels during the training process
UpdateLabel = True
# Whether save trajectory results
SaveTrajResults = True


[Data]

